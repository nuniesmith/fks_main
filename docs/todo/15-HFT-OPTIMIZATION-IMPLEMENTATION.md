# HFT Optimization Implementation - Complete Task Guide
## AI Agent Instructions for Ultra-Low-Latency Trading Enhancements

**Date**: 2025-01-XX  
**Status**: Ready for Implementation  
**Purpose**: Step-by-step guide for AI agents to implement HFT optimizations (kernel-bypass, FPGA, in-memory order books, SOR)  
**Estimated Effort**: 300-400 hours over 6-8 weeks  
**Prerequisites**: Phase 3-4 of multi-agent bots (14-MULTI-AGENT-TRADING-BOTS-IMPLEMENTATION.md)

---

## ðŸŽ¯ Project Overview

**Objective**: Integrate High-Frequency Trading (HFT) optimizations into FKS platform to achieve microsecond-to-nanosecond latency, enabling competitive trading in volatile markets while maintaining service boundaries and BTC-centric portfolio focus.

**Key Deliverables**:
1. Kernel-bypass networking (DPDK) for fks_execution (50% latency reduction)
2. FPGA acceleration for fks_ai (nanosecond decisions, 20-30% accuracy gain)
3. In-memory order books with lock-free structures (25% redundancy boost)
4. Smart Order Router (SOR) with AI-driven venue selection (30% better fill rates)
5. Hardware timestamping and latency monitoring (sub-microsecond tracking)
6. Event-driven pipelines (LMAX Disruptor) for lock-free queues

**Success Criteria**:
- Signal-to-execution latency: <1Î¼s (from milliseconds)
- Data ingestion latency: <100ns (from microseconds)
- System throughput: 1M+ trades/second
- FPGA arbitrage detection: <500ns
- 95% latency reduction in critical paths

---

## ðŸ“‹ Phase 1: Preparation and Feasibility (Week 1)

### Task 1.1: HFT Architecture Analysis

**Objective**: Analyze current FKS architecture and identify latency bottlenecks

**Actions for AI Agent**:

1. **Create latency profiling script**:
   ```
   File: repo/monitor/src/latency/profiler.py
   Content:
   import time
   import statistics
   from typing import List, Dict, Any
   from loguru import logger
   import asyncio
   import httpx
   
   class LatencyProfiler:
       """Profile latency across FKS services"""
       
       def __init__(self):
           self.metrics = {}
           self.histogram = {}
       
       async def profile_service(self, service_name: str, endpoint: str, iterations: int = 1000):
           """Profile service endpoint latency"""
           latencies = []
           
           async with httpx.AsyncClient() as client:
               for i in range(iterations):
                   start = time.perf_counter_ns()  # Nanosecond precision
                   try:
                       response = await client.get(endpoint, timeout=1.0)
                       end = time.perf_counter_ns()
                       latency_ns = end - start
                       latencies.append(latency_ns)
                   except Exception as e:
                       logger.error(f"Error profiling {service_name}: {e}")
           
           if latencies:
               self.metrics[service_name] = {
                   "mean": statistics.mean(latencies) / 1e6,  # Convert to ms
                   "median": statistics.median(latencies) / 1e6,
                   "p95": statistics.quantiles(latencies, n=20)[18] / 1e6,
                   "p99": statistics.quantiles(latencies, n=100)[98] / 1e6,
                   "min": min(latencies) / 1e6,
                   "max": max(latencies) / 1e6,
                   "std": statistics.stdev(latencies) / 1e6 if len(latencies) > 1 else 0
               }
               logger.info(f"{service_name} latency: {self.metrics[service_name]}")
           
           return self.metrics.get(service_name, {})
       
       async def profile_full_workflow(self):
           """Profile end-to-end signal generation to execution"""
           workflow_steps = [
               ("fks_data", "http://fks_data:8003/api/v1/data/BTC-USD"),
               ("fks_ai", "http://fks_ai:8007/ai/bots/consensus?symbol=BTC-USD"),
               ("fks_portfolio", "http://fks_portfolio:8012/api/signals/bot-signals"),
               ("fks_execution", "http://fks_execution:8006/api/execution/health")
           ]
           
           total_start = time.perf_counter_ns()
           
           for service, endpoint in workflow_steps:
               await self.profile_service(service, endpoint)
           
           total_end = time.perf_counter_ns()
           total_latency = (total_end - total_start) / 1e6
           
           logger.info(f"Total workflow latency: {total_latency:.2f}ms")
           
           return {
               "total_latency_ms": total_latency,
               "service_metrics": self.metrics
           }
   
   # CLI script
   if __name__ == "__main__":
       import asyncio
       profiler = LatencyProfiler()
       asyncio.run(profiler.profile_full_workflow())
   ```

2. **Run baseline latency profiling**:
   ```bash
   cd repo/monitor
   python src/latency/profiler.py > baseline-latency-report.json
   ```

3. **Create bottleneck analysis document**:
   ```
   File: repo/main/docs/hft-bottleneck-analysis.md
   Content:
   # HFT Bottleneck Analysis
   
   ## Current Latency Profile
   [Include results from profiler]
   
   ## Identified Bottlenecks
   1. **fks_execution**: OS kernel network stack (5-10ms)
   2. **fks_data**: HTTP request overhead (2-5ms)
   3. **fks_ai**: GPU inference latency (10-50ms)
   4. **fks_portfolio**: Python GIL limitations (1-3ms)
   
   ## Target Latencies
   - Data ingestion: <100ns (from 2-5ms)
   - Signal generation: <1Î¼s (from 10-50ms with FPGA)
   - Order execution: <500ns (from 5-10ms)
   - End-to-end: <10Î¼s (from 20-70ms)
   
   ## Optimization Opportunities
   - Kernel-bypass (DPDK): 40-50% reduction
   - FPGA acceleration: 20-30% accuracy + 1000x speedup
   - In-memory order books: 25% redundancy + faster access
   - Lock-free queues: Eliminate contention
   ```

**Deliverable**: Baseline latency profile and bottleneck analysis

**Success Criteria**: All bottlenecks identified, target latencies defined

---

### Task 1.2: Hardware Requirements Assessment

**Objective**: Determine hardware requirements for HFT optimizations

**Actions for AI Agent**:

1. **Create hardware requirements document**:
   ```
   File: repo/main/docs/hft-hardware-requirements.md
   Content:
   # HFT Hardware Requirements
   
   ## FPGA Requirements
   - **Type**: Xilinx Zynq UltraScale+ or Intel Stratix 10
   - **Logic Cells**: 1M+ for complex strategies
   - **Memory**: 32GB+ DDR4 on-board
   - **Interfaces**: PCIe 4.0 x16, 10GbE/100GbE NICs
   - **Development**: Vivado (Xilinx) or Quartus (Intel)
   - **Cost**: $5K-$50K per FPGA card
   
   ## Network Requirements
   - **NICs**: Solarflare X2 or Mellanox ConnectX-6 (kernel-bypass capable)
   - **Bandwidth**: 10GbE minimum, 100GbE preferred
   - **Latency**: <1Î¼s port-to-port
   - **Features**: Hardware timestamping, RDMA support
   - **Cost**: $500-$2K per NIC
   
   ## Server Requirements
   - **CPU**: Intel Xeon Scalable or AMD EPYC (high clock speed)
   - **RAM**: 128GB+ DDR4 (low latency, ECC)
   - **Storage**: NVMe SSD for order book persistence
   - **OS**: Linux (Ubuntu 22.04 LTS or RHEL 9) with real-time kernel
   - **Colocation**: Proximity to exchange data centers (<1ms)
   
   ## Development Environment
   - **FPGA Toolchain**: Vivado 2024.1+ or Quartus Prime 23.1+
   - **DPDK**: 23.11+ (kernel-bypass framework)
   - **Rust**: 1.75+ (for low-latency code)
   - **C++**: GCC 13+ with -O3 -march=native optimizations
   
   ## Cloud vs On-Premise
   - **Cloud**: AWS EC2 F1 instances (FPGA), but higher latency
   - **On-Premise**: Colocated servers with FPGA cards (recommended)
   - **Hybrid**: Development in cloud, production on-premise
   ```

2. **Create hardware procurement checklist**:
   ```
   File: repo/main/docs/hft-procurement-checklist.md
   Content:
   # HFT Hardware Procurement Checklist
   
   ## Phase 1: Development (Week 1-2)
   - [ ] FPGA development board (Xilinx ZCU104 or Intel Arria 10)
   - [ ] 10GbE NICs (2x for testing)
   - [ ] Development server (64GB RAM, high clock CPU)
   - [ ] FPGA toolchain licenses
   
   ## Phase 2: Staging (Week 3-4)
   - [ ] Production FPGA cards (2x for redundancy)
   - [ ] 100GbE NICs (4x for high throughput)
   - [ ] Staging servers (colocated if possible)
   - [ ] Network switches (low-latency, <1Î¼s)
   
   ## Phase 3: Production (Week 5-6)
   - [ ] Production servers (colocated near exchanges)
   - [ ] Redundant FPGA cards
   - [ ] Monitoring hardware (oscilloscope for latency measurement)
   - [ ] Backup power (UPS + generator)
   ```

**Deliverable**: Hardware requirements and procurement checklist

**Success Criteria**: Hardware requirements defined, procurement plan ready

---

### Task 1.3: FPGA Feasibility Study

**Objective**: Evaluate FPGA feasibility for fks_ai strategies

**Actions for AI Agent**:

1. **Create FPGA strategy analysis**:
   ```
   File: repo/ai/docs/fpga-feasibility-study.md
   Content:
   # FPGA Feasibility Study for fks_ai
   
   ## Strategies Suitable for FPGA
   1. **Arbitrage Detection** (High Priority)
      - Logic: Compare prices across exchanges
      - Latency: <500ns achievable
      - Complexity: Medium (5K-10K LUTs)
      - ROI: High (immediate profit on price differences)
   
   2. **Order Book Matching** (High Priority)
      - Logic: Lock-free order book maintenance
      - Latency: <100ns per update
      - Complexity: High (20K-50K LUTs)
      - ROI: Very High (core trading functionality)
   
   3. **Signal Generation** (Medium Priority)
      - Logic: Technical indicators (EMA, RSI, MACD)
      - Latency: <1Î¼s
      - Complexity: Low-Medium (2K-5K LUTs)
      - ROI: Medium (faster signal generation)
   
   4. **Risk Checks** (Medium Priority)
      - Logic: Pre-trade risk validation
      - Latency: <200ns
      - Complexity: Low (1K-2K LUTs)
      - ROI: High (prevents losses)
   
   ## Strategies NOT Suitable for FPGA
   - LLM inference (too complex, use GPU)
   - RAG queries (use CPU/GPU)
   - Multi-agent debates (use CPU with LangGraph)
   
   ## Implementation Approach
   - **Phase 1**: Arbitrage detection (simplest, highest ROI)
   - **Phase 2**: Order book matching (core functionality)
   - **Phase 3**: Signal generation (optimization)
   - **Phase 4**: Risk checks (safety)
   
   ## FPGA vs GPU Comparison
   | Metric | FPGA | GPU |
   |--------|------|-----|
   | Latency | <1Î¼s | 1-10ms |
   | Power | 50-100W | 200-400W |
   | Flexibility | Low (recompile) | High (runtime) |
   | Cost | $5K-$50K | $1K-$10K |
   | Best For | Deterministic, low-latency | ML inference, parallel |
   ```

2. **Create FPGA prototype specification**:
   ```
   File: repo/ai/docs/fpga-prototype-spec.md
   Content:
   # FPGA Prototype Specification: Arbitrage Detection
   
   ## Functionality
   - Input: Price feeds from multiple exchanges (via fks_data)
   - Output: Arbitrage opportunities (symbol, buy_exchange, sell_exchange, profit)
   - Latency: <500ns
   - Throughput: 1M+ price updates/second
   
   ## Interface
   - **Input**: 64-bit price data (symbol: 16-bit, price: 48-bit)
   - **Output**: 128-bit arbitrage signal (symbol, buy_ex, sell_ex, profit, timestamp)
   - **Clock**: 200MHz (5ns period)
   - **Pipeline Stages**: 10-15 stages
   
   ## Algorithm
   1. Receive price update from exchange A
   2. Compare with cached prices from exchanges B, C, D
   3. Calculate profit (accounting for fees)
   4. If profit > threshold, generate arbitrage signal
   5. Update price cache
   
   ## Resource Estimation
   - **LUTs**: 8,000-12,000
   - **FFs**: 10,000-15,000
   - **BRAM**: 50-100 blocks (for price cache)
   - **DSP**: 20-30 (for profit calculations)
   ```

**Deliverable**: FPGA feasibility study and prototype specification

**Success Criteria**: FPGA strategy defined, prototype spec complete

---

## ðŸ“‹ Phase 2: Kernel-Bypass Networking (DPDK) - Weeks 2-3

### Task 2.1: Install and Configure DPDK

**Objective**: Set up DPDK for kernel-bypass networking in fks_execution

**Actions for AI Agent**:

1. **Create DPDK installation script**:
   ```
   File: repo/execution/scripts/install-dpdk.sh
   Content:
   #!/bin/bash
   # Install DPDK for kernel-bypass networking
   
   set -e
   
   DPDK_VERSION="23.11"
   DPDK_DIR="/opt/dpdk"
   
   echo "=== Installing DPDK $DPDK_VERSION ==="
   
   # Install dependencies
   sudo apt-get update
   sudo apt-get install -y \
       build-essential \
       libnuma-dev \
       libpcap-dev \
       python3-pip \
       meson \
       ninja-build
   
   # Download DPDK
   cd /tmp
   wget https://fast.dpdk.org/rel/dpdk-${DPDK_VERSION}.tar.xz
   tar xf dpdk-${DPDK_VERSION}.tar.xz
   cd dpdk-${DPDK_VERSION}
   
   # Configure and build
   meson setup build
   cd build
   ninja
   sudo ninja install
   
   # Set up hugepages (required for DPDK)
   echo "Setting up hugepages..."
   sudo sysctl -w vm.nr_hugepages=1024
   echo "vm.nr_hugepages=1024" | sudo tee -a /etc/sysctl.conf
   
   # Bind NIC to DPDK (replace enp0s8 with your NIC)
   echo "Binding NIC to DPDK..."
   sudo modprobe uio
   sudo modprobe igb_uio
   sudo ./usertools/dpdk-devbind.py --bind=igb_uio enp0s8
   
   echo "=== DPDK installed successfully ==="
   echo "Verify with: sudo ./usertools/dpdk-devbind.py --status"
   ```

2. **Create DPDK configuration**:
   ```
   File: repo/execution/config/dpdk-config.toml
   Content:
   [dpdk]
   # DPDK configuration
   hugepage_dir = "/mnt/huge"
   memory_channels = 2
   cores = [0, 1, 2, 3]  # CPU cores for DPDK
   
   [network]
   # Network configuration
   port_id = 0
   rx_queues = 4
   tx_queues = 4
   rx_queue_size = 1024
   tx_queue_size = 1024
   
   [performance]
   # Performance tuning
   no_shconf = true
   no_huge = false
   no_pci = false
   no_hpet = false
   no_shconf = true
   vfio_intr = "legacy"
   ```

3. **Create Rust DPDK bindings** (if not available):
   ```
   File: repo/execution/src/dpdk/bindings.rs
   Content:
   // Rust bindings for DPDK
   // Use dpdk-sys crate or generate with bindgen
   
   use std::ffi::CString;
   use std::os::raw::c_int;
   
   #[link(name = "dpdk")]
   extern "C" {
       pub fn rte_eal_init(argc: c_int, argv: *const *const i8) -> c_int;
       pub fn rte_eth_dev_count_avail() -> u16;
       pub fn rte_eth_rx_burst(port_id: u16, queue_id: u16, rx_pkts: *mut *mut u8, nb_pkts: u16) -> u16;
       pub fn rte_eth_tx_burst(port_id: u16, queue_id: u16, tx_pkts: *const *const u8, nb_pkts: u16) -> u16;
   }
   
   pub struct DPDKRuntime {
       port_id: u16,
   }
   
   impl DPDKRuntime {
       pub fn new() -> Result<Self, String> {
           unsafe {
               let args = vec![
                   CString::new("fks_execution").unwrap(),
                   CString::new("-l").unwrap(),
                   CString::new("0-3").unwrap(),
                   CString::new("--huge-dir").unwrap(),
                   CString::new("/mnt/huge").unwrap(),
               ];
               let argc = args.len() as c_int;
               let argv: Vec<*const i8> = args.iter().map(|s| s.as_ptr()).collect();
               
               let ret = rte_eal_init(argc, argv.as_ptr());
               if ret < 0 {
                   return Err("DPDK initialization failed".to_string());
               }
           }
           
           Ok(DPDKRuntime { port_id: 0 })
       }
       
       pub fn receive_packets(&mut self, queue_id: u16, buffer: &mut [*mut u8], max_packets: u16) -> usize {
           unsafe {
               rte_eth_rx_burst(self.port_id, queue_id, buffer.as_mut_ptr(), max_packets) as usize
           }
       }
       
       pub fn send_packets(&mut self, queue_id: u16, packets: &[*const u8], num_packets: u16) -> usize {
           unsafe {
               rte_eth_tx_burst(self.port_id, queue_id, packets.as_ptr(), num_packets) as usize
           }
       }
   }
   ```

4. **Update Cargo.toml**:
   ```
   File: repo/execution/Cargo.toml
   # Add DPDK dependency
   [dependencies]
   dpdk-sys = "0.1"  # Or use bindgen to generate bindings
   # ... other dependencies
   ```

**Deliverable**: DPDK installed and configured, Rust bindings created

**Success Criteria**: DPDK running, NIC bound, packets can be received/sent

---

### Task 2.2: Implement DPDK Data Path in fks_execution

**Objective**: Replace HTTP/WebSocket with DPDK for ultra-low-latency data reception

**Actions for AI Agent**:

1. **Create DPDK data receiver**:
   ```
   File: repo/execution/src/dpdk/data_receiver.rs
   Content:
   use std::time::{Duration, Instant};
   use log::info;
   use crate::dpdk::bindings::DPDKRuntime;
   
   pub struct DPDKDataReceiver {
       dpdk: DPDKRuntime,
       queue_id: u16,
       buffer: Vec<*mut u8>,
   }
   
   impl DPDKDataReceiver {
       pub fn new(queue_id: u16) -> Result<Self, String> {
           let dpdk = DPDKRuntime::new()?;
           let buffer = vec![std::ptr::null_mut(); 1024];
           
           Ok(DPDKDataReceiver {
               dpdk,
               queue_id,
               buffer,
           })
       }
       
       pub fn receive_market_data(&mut self) -> Vec<MarketDataPacket> {
           let start = Instant::now();
           let num_packets = self.dpdk.receive_packets(
               self.queue_id,
               &mut self.buffer,
               1024
           );
           
           let mut packets = Vec::new();
           for i in 0..num_packets {
               unsafe {
                   let packet = self.buffer[i];
                   // Parse packet (assuming custom protocol)
                   let market_data = self.parse_packet(packet);
                   packets.push(market_data);
               }
           }
           
           let latency = start.elapsed();
           if latency.as_nanos() > 1000 {
               info!("DPDK receive latency: {}ns", latency.as_nanos());
           }
           
           packets
       }
       
       fn parse_packet(&self, packet: *mut u8) -> MarketDataPacket {
           // Parse custom market data protocol
           // Format: [symbol: 16 bytes][price: 8 bytes][timestamp: 8 bytes][volume: 8 bytes]
           unsafe {
               let symbol = std::str::from_utf8_unchecked(
                   std::slice::from_raw_parts(packet, 16)
               ).trim_end_matches('\0');
               let price = f64::from_be_bytes(
                   *std::slice::from_raw_parts(packet.add(16), 8).as_ptr() as *const [u8; 8]
               );
               let timestamp = u64::from_be_bytes(
                   *std::slice::from_raw_parts(packet.add(24), 8).as_ptr() as *const [u8; 8]
               );
               let volume = f64::from_be_bytes(
                   *std::slice::from_raw_parts(packet.add(32), 8).as_ptr() as *const [u8; 8]
               );
               
               MarketDataPacket {
                   symbol: symbol.to_string(),
                   price,
                   timestamp,
                   volume,
               }
           }
       }
   }
   
   #[derive(Debug, Clone)]
   pub struct MarketDataPacket {
       pub symbol: String,
       pub price: f64,
       pub timestamp: u64,
       pub volume: f64,
   }
   ```

2. **Integrate DPDK receiver into execution service**:
   ```
   File: repo/execution/src/execution/dpdk_execution.rs
   Content:
   use crate::dpdk::data_receiver::DPDKDataReceiver;
   use crate::order::Order;
   use std::sync::mpsc;
   use std::thread;
   
   pub struct DPDKExecutionEngine {
       receiver: DPDKDataReceiver,
       order_tx: mpsc::Sender<Order>,
   }
   
   impl DPDKExecutionEngine {
       pub fn new(order_tx: mpsc::Sender<Order>) -> Result<Self, String> {
           let receiver = DPDKDataReceiver::new(0)?;
           
           Ok(DPDKExecutionEngine {
               receiver,
               order_tx,
           })
       }
       
       pub fn run(&mut self) {
           loop {
               let packets = self.receiver.receive_market_data();
               
               for packet in packets {
                   // Process market data and trigger orders
                   if self.should_execute_order(&packet) {
                       let order = self.create_order(&packet);
                       self.order_tx.send(order).unwrap();
                   }
               }
           }
       }
       
       fn should_execute_order(&self, packet: &MarketDataPacket) -> bool {
           // Implement order triggering logic
           // Check signals from fks_ai, risk limits, etc.
           true
       }
       
       fn create_order(&self, packet: &MarketDataPacket) -> Order {
           // Create order from market data
           Order {
               symbol: packet.symbol.clone(),
               side: "BUY".to_string(),
               quantity: 1.0,
               price: packet.price,
               timestamp: packet.timestamp,
           }
       }
   }
   ```

3. **Create performance benchmarks**:
   ```
   File: repo/execution/benches/dpdk_benchmark.rs
   Content:
   use criterion::{black_box, criterion_group, criterion_main, Criterion};
   use fks_execution::dpdk::data_receiver::DPDKDataReceiver;
   
   fn benchmark_dpdk_receive(c: &mut Criterion) {
       let mut receiver = DPDKDataReceiver::new(0).unwrap();
       
       c.bench_function("dpdk_receive_packets", |b| {
           b.iter(|| {
               let packets = receiver.receive_market_data();
               black_box(packets);
           });
       });
   }
   
   criterion_group!(benches, benchmark_dpdk_receive);
   criterion_main!(benches);
   ```

**Deliverable**: DPDK data path implemented in fks_execution

**Success Criteria**: Packets received via DPDK, latency <1Î¼s, benchmarks show 40-50% improvement

---

### Task 2.3: Update fks_data for DPDK Transmission

**Objective**: Modify fks_data to transmit market data via DPDK instead of HTTP

**Actions for AI Agent**:

1. **Create DPDK transmitter in fks_data**:
   ```
   File: repo/data/src/dpdk/transmitter.py
   Content:
   import ctypes
   import struct
   from typing import List, Dict, Any
   from loguru import logger
   
   class DPDKTransmitter:
       """Transmit market data via DPDK"""
       
       def __init__(self, port_id: int = 0):
           # Load DPDK library
           self.dpdk_lib = ctypes.CDLL("libdpdk.so")
           self.port_id = port_id
           self.initialized = False
       
       def initialize(self):
           """Initialize DPDK"""
           # Initialize DPDK EAL
           args = [
               "fks_data".encode(),
               "-l".encode(),
               "0-3".encode(),
               "--huge-dir".encode(),
               "/mnt/huge".encode(),
           ]
           argc = len(args)
           argv = (ctypes.POINTER(ctypes.c_char) * argc)(*[ctypes.c_char_p(arg) for arg in args])
           
           ret = self.dpdk_lib.rte_eal_init(argc, argv)
           if ret < 0:
               raise RuntimeError("DPDK initialization failed")
           
           self.initialized = True
           logger.info("DPDK transmitter initialized")
       
       def transmit_market_data(self, symbol: str, price: float, timestamp: int, volume: float):
           """Transmit market data packet via DPDK"""
           if not self.initialized:
               self.initialize()
           
           # Create packet: [symbol: 16 bytes][price: 8 bytes][timestamp: 8 bytes][volume: 8 bytes]
           packet = bytearray(40)
           
           # Pack symbol (16 bytes, null-padded)
           symbol_bytes = symbol.encode()[:16].ljust(16, b'\0')
           packet[0:16] = symbol_bytes
           
           # Pack price (8 bytes, double)
           packet[16:24] = struct.pack('>d', price)
           
           # Pack timestamp (8 bytes, uint64)
           packet[24:32] = struct.pack('>Q', timestamp)
           
           # Pack volume (8 bytes, double)
           packet[32:40] = struct.pack('>d', volume)
           
           # Transmit via DPDK
           packet_ptr = ctypes.c_void_p(ctypes.addressof(ctypes.c_char_p(packet)))
           ret = self.dpdk_lib.rte_eth_tx_burst(
               self.port_id,
               0,  # queue_id
               ctypes.byref(packet_ptr),
               1   # num_packets
           )
           
           if ret != 1:
               logger.warning(f"Failed to transmit packet: {ret}")
       
       def transmit_batch(self, data_points: List[Dict[str, Any]]):
           """Transmit batch of market data"""
           for data in data_points:
               self.transmit_market_data(
                   data["symbol"],
                   data["price"],
                   data["timestamp"],
                   data["volume"]
               )
   ```

2. **Integrate DPDK transmitter into data adapters**:
   ```
   File: repo/data/src/adapters/binance_adapter.py
   # Add DPDK transmission option
   
   from ..dpdk.transmitter import DPDKTransmitter
   
   class BinanceAdapter:
       def __init__(self, use_dpdk: bool = False):
           # ... existing code ...
           self.dpdk_transmitter = DPDKTransmitter() if use_dpdk else None
           if use_dpdk:
               self.dpdk_transmitter.initialize()
       
       async def fetch_and_transmit(self, symbol: str):
           """Fetch data and transmit via DPDK"""
           data = await self.fetch_data(symbol)
           
           if self.dpdk_transmitter:
               # Transmit via DPDK (ultra-low latency)
               self.dpdk_transmitter.transmit_market_data(
                   symbol,
                   data["price"],
                   data["timestamp"],
                   data["volume"]
               )
           else:
               # Store in database (existing behavior)
               await self.store_data(data)
   ```

**Deliverable**: fks_data transmits market data via DPDK

**Success Criteria**: Data transmitted via DPDK, latency <100ns, no packet loss

---

## ðŸ“‹ Phase 3: FPGA Acceleration - Weeks 4-5

### Task 3.1: FPGA Development Environment Setup

**Objective**: Set up FPGA toolchain and development environment

**Actions for AI Agent**:

1. **Create FPGA development setup guide**:
   ```
   File: repo/ai/docs/fpga-setup-guide.md
   Content:
   # FPGA Development Environment Setup
   
   ## Prerequisites
   - Xilinx Vivado 2024.1+ or Intel Quartus Prime 23.1+
   - FPGA development board (Xilinx ZCU104 or Intel Arria 10)
   - License for synthesis tools
   
   ## Installation Steps
   1. Install Vivado/Quartus
   2. Set up license server
   3. Configure board files
   4. Test with simple LED blink project
   
   ## Project Structure
   ```
   repo/ai/fpga/
   â”œâ”€â”€ projects/
   â”‚   â”œâ”€â”€ arbitrage_detector/
   â”‚   â”‚   â”œâ”€â”€ src/
   â”‚   â”‚   â”‚   â”œâ”€â”€ arbitrage.v (Verilog)
   â”‚   â”‚   â”‚   â”œâ”€â”€ price_cache.v
   â”‚   â”‚   â”‚   â””â”€â”€ profit_calculator.v
   â”‚   â”‚   â”œâ”€â”€ constraints/
   â”‚   â”‚   â”‚   â””â”€â”€ timing.xdc
   â”‚   â”‚   â””â”€â”€ testbench/
   â”‚   â”‚       â””â”€â”€ arbitrage_tb.v
   â”‚   â””â”€â”€ order_book/
   â”‚       â””â”€â”€ ...
   â”œâ”€â”€ scripts/
   â”‚   â”œâ”€â”€ build.sh
   â”‚   â””â”€â”€ program.sh
   â””â”€â”€ README.md
   ```
   ```

2. **Create FPGA build script**:
   ```
   File: repo/ai/fpga/scripts/build.sh
   Content:
   #!/bin/bash
   # Build FPGA project
   
   set -e
   
   PROJECT_NAME="arbitrage_detector"
   PROJECT_DIR="projects/${PROJECT_NAME}"
   
   echo "=== Building FPGA project: ${PROJECT_NAME} ==="
   
   # Source Vivado environment
   source /opt/Xilinx/Vivado/2024.1/settings64.sh
   
   # Create project
   vivado -mode batch -source scripts/create_project.tcl -tclargs ${PROJECT_NAME}
   
   # Synthesize
   vivado -mode batch -source scripts/synthesize.tcl -tclargs ${PROJECT_NAME}
   
   # Implement
   vivado -mode batch -source scripts/implement.tcl -tclargs ${PROJECT_NAME}
   
   # Generate bitstream
   vivado -mode batch -source scripts/generate_bitstream.tcl -tclargs ${PROJECT_NAME}
   
   echo "=== FPGA build complete ==="
   ```

3. **Create Verilog arbitrage detector**:
   ```
   File: repo/ai/fpga/projects/arbitrage_detector/src/arbitrage.v
   Content:
   // Arbitrage Detection Module
   // Detects price differences across exchanges
   // Latency: <500ns @ 200MHz
   
   module arbitrage_detector (
       input wire clk,
       input wire rst,
       input wire [15:0] symbol,
       input wire [47:0] price_in,
       input wire [1:0] exchange_id,
       input wire price_valid,
       output reg [15:0] arb_symbol,
       output reg [1:0] buy_exchange,
       output reg [1:0] sell_exchange,
       output reg [47:0] profit,
       output reg arb_valid
   );
   
       // Price cache for 4 exchanges
       reg [47:0] price_cache [0:3] [0:65535];  // [exchange][symbol]
       reg [63:0] timestamp_cache [0:3] [0:65535];
       
       // Pipeline registers
       reg [15:0] symbol_reg;
       reg [47:0] price_reg;
       reg [1:0] exchange_reg;
       reg price_valid_reg;
       
       // Profit calculation
       reg [47:0] buy_price;
       reg [47:0] sell_price;
       reg [47:0] profit_calc;
       reg [47:0] fee = 48'd100;  // 0.1% fee (scaled)
       
       always @(posedge clk) begin
           if (rst) begin
               arb_valid <= 1'b0;
               price_valid_reg <= 1'b0;
           end else begin
               // Stage 1: Cache price update
               if (price_valid) begin
                   price_cache[exchange_id][symbol] <= price_in;
                   timestamp_cache[exchange_id][symbol] <= $time;
                   symbol_reg <= symbol;
                   price_reg <= price_in;
                   exchange_reg <= exchange_id;
                   price_valid_reg <= 1'b1;
               end else begin
                   price_valid_reg <= 1'b0;
               end
               
               // Stage 2: Compare with other exchanges
               if (price_valid_reg) begin
                   // Find best buy and sell prices
                   buy_price <= price_cache[0][symbol_reg];
                   sell_price <= price_cache[1][symbol_reg];
                   
                   // Compare all exchanges
                   if (price_cache[2][symbol_reg] < buy_price)
                       buy_price <= price_cache[2][symbol_reg];
                   if (price_cache[3][symbol_reg] < buy_price)
                       buy_price <= price_cache[3][symbol_reg];
                   
                   if (price_cache[2][symbol_reg] > sell_price)
                       sell_price <= price_cache[2][symbol_reg];
                   if (price_cache[3][symbol_reg] > sell_price)
                       sell_price <= price_cache[3][symbol_reg];
               end
               
               // Stage 3: Calculate profit
               if (price_valid_reg) begin
                   // Profit = (sell_price - buy_price) - fees
                   profit_calc <= (sell_price - buy_price) - (fee * 2);
                   
                   // Check if profit > threshold
                   if (profit_calc > 48'd1000) begin  // 0.1% profit threshold
                       arb_symbol <= symbol_reg;
                       buy_exchange <= 2'b00;  // Simplified, should track which exchange
                       sell_exchange <= 2'b01;
                       profit <= profit_calc;
                       arb_valid <= 1'b1;
                   end else begin
                       arb_valid <= 1'b0;
                   end
               end
           end
       end
   endmodule
   ```

4. **Create testbench**:
   ```
   File: repo/ai/fpga/projects/arbitrage_detector/testbench/arbitrage_tb.v
   Content:
   `timescale 1ns / 1ps
   
   module arbitrage_tb;
       reg clk;
       reg rst;
       reg [15:0] symbol;
       reg [47:0] price_in;
       reg [1:0] exchange_id;
       reg price_valid;
       wire [15:0] arb_symbol;
       wire [1:0] buy_exchange;
       wire [1:0] sell_exchange;
       wire [47:0] profit;
       wire arb_valid;
       
       arbitrage_detector uut (
           .clk(clk),
           .rst(rst),
           .symbol(symbol),
           .price_in(price_in),
           .exchange_id(exchange_id),
           .price_valid(price_valid),
           .arb_symbol(arb_symbol),
           .buy_exchange(buy_exchange),
           .sell_exchange(sell_exchange),
           .profit(profit),
           .arb_valid(arb_valid)
       );
       
       initial begin
           clk = 0;
           forever #2.5 clk = ~clk;  // 200MHz clock
       end
       
       initial begin
           rst = 1;
           #100;
           rst = 0;
           
           // Test case: Price difference between exchanges
           symbol = 16'h0001;  // BTC-USD
           price_in = 48'd50000;  // $50,000
           exchange_id = 2'b00;  // Exchange 0
           price_valid = 1;
           #10;
           price_valid = 0;
           
           #100;
           
           price_in = 48'd50100;  // $50,100 (higher price)
           exchange_id = 2'b01;  // Exchange 1
           price_valid = 1;
           #10;
           price_valid = 0;
           
           #100;
           
           // Should detect arbitrage opportunity
           if (arb_valid && profit > 48'd1000) begin
               $display("Arbitrage detected: Profit = %d", profit);
           end else begin
               $display("ERROR: Arbitrage not detected");
           end
           
           #1000;
           $finish;
       end
   endmodule
   ```

**Deliverable**: FPGA development environment set up, arbitrage detector implemented

**Success Criteria**: FPGA project builds, testbench passes, latency <500ns verified

---

### Task 3.2: Integrate FPGA with fks_ai

**Objective**: Connect FPGA arbitrage detector to fks_ai service

**Actions for AI Agent**:

1. **Create FPGA driver**:
   ```
   File: repo/ai/src/fpga/driver.py
   Content:
   import mmap
   import struct
   import os
   from typing import Optional, Dict, Any
   from loguru import logger
   
   class FPGADriver:
       """Driver for FPGA arbitrage detector"""
       
       def __init__(self, device_path: str = "/dev/uio0"):
           self.device_path = device_path
           self.mmap_size = 4096  # 4KB memory map
           self.mmap = None
           self.device_fd = None
       
       def open(self):
           """Open FPGA device"""
           try:
               self.device_fd = os.open(self.device_path, os.O_RDWR)
               self.mmap = mmap.mmap(
                   self.device_fd,
                   self.mmap_size,
                   mmap.MAP_SHARED,
                   mmap.PROT_READ | mmap.PROT_WRITE
               )
               logger.info(f"FPGA device opened: {self.device_path}")
           except Exception as e:
               logger.error(f"Failed to open FPGA device: {e}")
               raise
       
       def write_price(self, symbol: int, price: float, exchange_id: int):
           """Write price to FPGA"""
           if not self.mmap:
               raise RuntimeError("FPGA device not open")
           
           # Pack data: [symbol: 2 bytes][price: 6 bytes][exchange: 1 byte][valid: 1 byte]
           symbol_bytes = struct.pack('>H', symbol)
           price_scaled = int(price * 1000)  # Scale to integer
           price_bytes = struct.pack('>Q', price_scaled)[2:8]  # 6 bytes
           exchange_byte = struct.pack('B', exchange_id)
           valid_byte = struct.pack('B', 1)
           
           # Write to FPGA memory map (offset 0)
           self.mmap[0:2] = symbol_bytes
           self.mmap[2:8] = price_bytes
           self.mmap[8:9] = exchange_byte
           self.mmap[9:10] = valid_byte
           
           # Trigger FPGA processing (write to control register)
           self.mmap[10:11] = struct.pack('B', 1)  # Start signal
       
       def read_arbitrage(self) -> Optional[Dict[str, Any]]:
           """Read arbitrage signal from FPGA"""
           if not self.mmap:
               return None
           
           # Read status register (offset 16)
           status = struct.unpack('B', self.mmap[16:17])[0]
           
           if status & 0x01:  # Arbitrage valid bit
               # Read arbitrage data (offset 20)
               symbol = struct.unpack('>H', self.mmap[20:22])[0]
               buy_exchange = struct.unpack('B', self.mmap[22:23])[0]
               sell_exchange = struct.unpack('B', self.mmap[23:24])[0]
               profit_scaled = struct.unpack('>Q', b'\x00\x00' + self.mmap[24:30])[0]
               profit = profit_scaled / 1000.0  # Unscale
               
               # Clear valid bit
               self.mmap[16:17] = struct.pack('B', status & ~0x01)
               
               return {
                   "symbol": symbol,
                   "buy_exchange": buy_exchange,
                   "sell_exchange": sell_exchange,
                   "profit": profit,
                   "timestamp": time.time_ns()
               }
           
           return None
       
       def close(self):
           """Close FPGA device"""
           if self.mmap:
               self.mmap.close()
           if self.device_fd:
               os.close(self.device_fd)
   ```

2. **Create FPGA arbitrage service**:
   ```
   File: repo/ai/src/fpga/arbitrage_service.py
   Content:
   import asyncio
   from typing import List, Dict, Any
   from loguru import logger
   from .driver import FPGADriver
   import httpx
   
   class FPGAArbitrageService:
       """Service for FPGA-based arbitrage detection"""
       
       def __init__(self):
           self.fpga = FPGADriver()
           self.fpga.open()
           self.running = False
           self.exchange_map = {
               "binance": 0,
               "coinbase": 1,
               "kraken": 2,
               "ftx": 3
           }
       
       async def start(self):
           """Start arbitrage detection loop"""
           self.running = True
           logger.info("FPGA arbitrage service started")
           
           while self.running:
               # Read arbitrage signals from FPGA
               arb_signal = self.fpga.read_arbitrage()
               
               if arb_signal:
                   logger.info(f"Arbitrage detected: {arb_signal}")
                   # Send to fks_portfolio for execution
                   await self.notify_portfolio(arb_signal)
               
               await asyncio.sleep(0.0001)  # 100Î¼s polling interval
       
       async def update_prices(self, symbol: str, prices: Dict[str, float]):
           """Update prices in FPGA"""
           symbol_id = self.get_symbol_id(symbol)
           
           for exchange, price in prices.items():
               exchange_id = self.exchange_map.get(exchange.lower(), 0)
               self.fpga.write_price(symbol_id, price, exchange_id)
       
       def get_symbol_id(self, symbol: str) -> int:
           """Convert symbol to ID"""
           # Simple hash function
           return hash(symbol) % 65536
       
       async def notify_portfolio(self, arb_signal: Dict[str, Any]):
           """Notify fks_portfolio of arbitrage opportunity"""
           try:
               async with httpx.AsyncClient() as client:
                   await client.post(
                       "http://fks_portfolio:8012/api/signals/arbitrage",
                       json=arb_signal,
                       timeout=1.0
                   )
           except Exception as e:
               logger.error(f"Failed to notify portfolio: {e}")
       
       def stop(self):
           """Stop arbitrage service"""
           self.running = False
           self.fpga.close()
   ```

3. **Create API endpoint**:
   ```
   File: repo/ai/src/api/fpga_routes.py
   Content:
   from fastapi import APIRouter, HTTPException
   from typing import Dict, Any
   from ..fpga.arbitrage_service import FPGAArbitrageService
   from loguru import logger
   
   router = APIRouter(prefix="/ai/fpga", tags=["fpga"])
   
   fpga_service = FPGAArbitrageService()
   
   @router.post("/arbitrage/update-prices")
   async def update_prices(request: Dict[str, Any]):
       """Update prices in FPGA"""
       try:
           symbol = request.get("symbol")
           prices = request.get("prices", {})
           
           await fpga_service.update_prices(symbol, prices)
           
           return {"status": "success"}
       except Exception as e:
           logger.error(f"FPGA update error: {e}")
           raise HTTPException(status_code=500, detail=str(e))
   
   @router.get("/arbitrage/status")
   async def get_status():
       """Get FPGA status"""
       return {
           "status": "running" if fpga_service.running else "stopped",
           "device": fpga_service.fpga.device_path
       }
   ```

4. **Start FPGA service on startup**:
   ```
   File: repo/ai/src/main.py
   # Add to startup
   
   from src.fpga.arbitrage_service import FPGAArbitrageService
   
   @app.on_event("startup")
   async def startup_event():
       # ... existing code ...
       
       # Start FPGA arbitrage service
       fpga_service = FPGAArbitrageService()
       asyncio.create_task(fpga_service.start())
   ```

**Deliverable**: FPGA integrated with fks_ai, arbitrage detection working

**Success Criteria**: FPGA detects arbitrage, latency <500ns, signals sent to portfolio

---

## ðŸ“‹ Phase 4: In-Memory Order Books - Week 6

### Task 4.1: Implement Lock-Free Order Book

**Objective**: Create in-memory order book with lock-free data structures

**Actions for AI Agent**:

1. **Create lock-free order book in Rust**:
   ```
   File: repo/execution/src/orderbook/lockfree_orderbook.rs
   Content:
   use std::sync::atomic::{AtomicPtr, AtomicU64, Ordering};
   use std::ptr;
   use std::alloc::{alloc, dealloc, Layout};
   
   // Lock-free skip list node
   struct OrderBookNode {
       price: AtomicU64,
       quantity: AtomicU64,
       next: AtomicPtr<OrderBookNode>,
       level: usize,
   }
   
   pub struct LockFreeOrderBook {
       bids: AtomicPtr<OrderBookNode>,  // Highest bid
       asks: AtomicPtr<OrderBookNode>,  // Lowest ask
       symbol: String,
   }
   
   impl LockFreeOrderBook {
       pub fn new(symbol: String) -> Self {
           LockFreeOrderBook {
               bids: AtomicPtr::new(ptr::null_mut()),
               asks: AtomicPtr::new(ptr::null_mut()),
               symbol,
           }
       }
       
       pub fn add_bid(&self, price: u64, quantity: u64) {
           let new_node = self.create_node(price, quantity);
           
           loop {
               let head = self.bids.load(Ordering::Acquire);
               
               // Find insertion point (descending order)
               let mut current = head;
               let mut prev = ptr::null_mut();
               
               while !current.is_null() {
                   unsafe {
                       let current_price = (*current).price.load(Ordering::Acquire);
                       if current_price < price {
                           break;  // Insert here
                       }
                       prev = current;
                       current = (*current).next.load(Ordering::Acquire);
                   }
               }
               
               // Set next pointer
               unsafe {
                   (*new_node).next.store(current, Ordering::Release);
               }
               
               // Try to insert
               if prev.is_null() {
                   // Insert at head
                   if self.bids.compare_exchange(
                       head,
                       new_node,
                       Ordering::Release,
                       Ordering::Relaxed
                   ).is_ok() {
                       return;
                   }
               } else {
                   // Insert after prev
                   unsafe {
                       let prev_next = (*prev).next.load(Ordering::Acquire);
                       if (*prev).next.compare_exchange(
                           prev_next,
                           new_node,
                           Ordering::Release,
                           Ordering::Relaxed
                       ).is_ok() {
                           return;
                       }
                   }
               }
               
               // Retry on failure
           }
       }
       
       pub fn add_ask(&self, price: u64, quantity: u64) {
           // Similar to add_bid, but ascending order
           // ... implementation ...
       }
       
       pub fn get_best_bid(&self) -> Option<(u64, u64)> {
           let head = self.bids.load(Ordering::Acquire);
           if head.is_null() {
               return None;
           }
           
           unsafe {
               Some((
                   (*head).price.load(Ordering::Acquire),
                   (*head).quantity.load(Ordering::Acquire)
               ))
           }
       }
       
       pub fn get_best_ask(&self) -> Option<(u64, u64)> {
           // Similar to get_best_bid
           // ... implementation ...
       }
       
       fn create_node(&self, price: u64, quantity: u64) -> *mut OrderBookNode {
           // Allocate node
           let layout = Layout::new::<OrderBookNode>();
           let node = unsafe { alloc(layout) as *mut OrderBookNode };
           
           unsafe {
               ptr::write(node, OrderBookNode {
                   price: AtomicU64::new(price),
                   quantity: AtomicU64::new(quantity),
                   next: AtomicPtr::new(ptr::null_mut()),
                   level: 0,
               });
           }
           
           node
       }
   }
   ```

2. **Create order book manager**:
   ```
   File: repo/execution/src/orderbook/manager.rs
   Content:
   use std::collections::HashMap;
   use std::sync::Arc;
   use tokio::sync::RwLock;
   use crate::orderbook::lockfree_orderbook::LockFreeOrderBook;
   
   pub struct OrderBookManager {
       order_books: Arc<RwLock<HashMap<String, Arc<LockFreeOrderBook>>>>,
   }
   
   impl OrderBookManager {
       pub fn new() -> Self {
           OrderBookManager {
               order_books: Arc::new(RwLock::new(HashMap::new())),
           }
       }
       
       pub async fn get_or_create(&self, symbol: String) -> Arc<LockFreeOrderBook> {
           let mut books = self.order_books.write().await;
           
           if let Some(book) = books.get(&symbol) {
               return Arc::clone(book);
           }
           
           let book = Arc::new(LockFreeOrderBook::new(symbol.clone()));
           books.insert(symbol, Arc::clone(&book));
           
           book
       }
       
       pub async fn update_order_book(
           &self,
           symbol: String,
           bids: Vec<(u64, u64)>,
           asks: Vec<(u64, u64)>
       ) {
           let book = self.get_or_create(symbol).await;
           
           // Update bids
           for (price, quantity) in bids {
               book.add_bid(price, quantity);
           }
           
           // Update asks
           for (price, quantity) in asks {
               book.add_ask(price, quantity);
           }
       }
   }
   ```

**Deliverable**: Lock-free order book implemented

**Success Criteria**: Order book updates <100ns, no locks, thread-safe

---

### Task 4.2: Integrate Order Book with fks_app

**Objective**: Use in-memory order book for backtesting and strategy optimization

**Actions for AI Agent**:

1. **Create Python bindings for order book**:
   ```
   File: repo/execution/src/orderbook/python_bindings.rs
   Content:
   use pyo3::prelude::*;
   use crate::orderbook::lockfree_orderbook::LockFreeOrderBook;
   
   #[pyclass]
   pub struct PyOrderBook {
       inner: LockFreeOrderBook,
   }
   
   #[pymethods]
   impl PyOrderBook {
       #[new]
       fn new(symbol: String) -> Self {
           PyOrderBook {
               inner: LockFreeOrderBook::new(symbol),
           }
       }
       
       fn add_bid(&self, price: f64, quantity: f64) {
           let price_scaled = (price * 1000.0) as u64;
           let quantity_scaled = (quantity * 1000.0) as u64;
           self.inner.add_bid(price_scaled, quantity_scaled);
       }
       
       fn get_best_bid(&self) -> Option<(f64, f64)> {
           self.inner.get_best_bid().map(|(p, q)| {
               (p as f64 / 1000.0, q as f64 / 1000.0)
           })
       }
   }
   
   #[pymodule]
   fn fks_orderbook(_py: Python, m: &PyModule) -> PyResult<()> {
       m.add_class::<PyOrderBook>()?;
       Ok(())
   }
   ```

2. **Integrate with fks_app backtesting**:
   ```
   File: repo/app/src/backtesting/orderbook_backtest.py
   Content:
   from fks_orderbook import PyOrderBook
   import pandas as pd
   
   class OrderBookBacktest:
       """Backtest using in-memory order book"""
       
       def __init__(self, symbol: str):
           self.order_book = PyOrderBook(symbol)
           self.symbol = symbol
       
       def run(self, data: pd.DataFrame):
           """Run backtest with order book"""
           for _, row in data.iterrows():
               # Update order book
               self.order_book.add_bid(row["bid_price"], row["bid_quantity"])
               self.order_book.add_ask(row["ask_price"], row["ask_quantity"])
               
               # Get best bid/ask
               best_bid = self.order_book.get_best_bid()
               best_ask = self.order_book.get_best_ask()
               
               # Execute strategy
               # ... strategy logic ...
   ```

**Deliverable**: Order book integrated with fks_app

**Success Criteria**: Backtesting 25% faster, order book updates in <100ns

---

## ðŸ“‹ Phase 5: Smart Order Router (SOR) - Week 7

### Task 5.1: Implement SOR with AI-Driven Venue Selection

**Objective**: Create Smart Order Router that optimizes venue selection based on latency and fill probability

**Actions for AI Agent**:

1. **Create SOR service**:
   ```
   File: repo/execution/src/sor/router.rs
   Content:
   use std::collections::HashMap;
   use std::time::{Duration, Instant};
   use serde::{Deserialize, Serialize};
   
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct Venue {
       pub name: String,
       pub latency_ms: f64,
       pub fill_probability: f64,
       pub fee_pct: f64,
       pub liquidity: f64,
   }
   
   pub struct SmartOrderRouter {
       venues: HashMap<String, Venue>,
       latency_history: HashMap<String, Vec<Duration>>,
   }
   
   impl SmartOrderRouter {
       pub fn new() -> Self {
           SmartOrderRouter {
               venues: HashMap::new(),
               latency_history: HashMap::new(),
           }
       }
       
       pub fn add_venue(&mut self, venue: Venue) {
           self.venues.insert(venue.name.clone(), venue);
       }
       
       pub fn select_venue(&self, symbol: String, quantity: f64, side: &str) -> Option<String> {
           // Score each venue
           let mut scores: Vec<(String, f64)> = Vec::new();
           
           for (name, venue) in &self.venues {
               // Calculate score: fill_probability * liquidity / (latency_ms + fee_pct)
               let score = (venue.fill_probability * venue.liquidity) 
                   / (venue.latency_ms + venue.fee_pct + 1.0);
               
               scores.push((name.clone(), score));
           }
           
           // Select venue with highest score
           scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
           
           scores.first().map(|(name, _)| name.clone())
       }
       
       pub fn update_latency(&mut self, venue: String, latency: Duration) {
           let history = self.latency_history.entry(venue).or_insert_with(Vec::new);
           history.push(latency);
           
           // Keep only last 100 measurements
           if history.len() > 100 {
               history.remove(0);
           }
           
           // Update venue latency (average of last 10)
           if let Some(venue_obj) = self.venues.get_mut(&venue) {
               let recent = history.iter().rev().take(10);
               let avg_latency = recent.sum::<Duration>() / recent.len() as u32;
               venue_obj.latency_ms = avg_latency.as_secs_f64() * 1000.0;
           }
       }
   }
   ```

2. **Integrate with fks_ai for AI-driven selection**:
   ```
   File: repo/ai/src/sor/ai_router.py
   Content:
   from typing import List, Dict, Any
   import httpx
   from loguru import logger
   
   class AIRouter:
       """AI-driven Smart Order Router"""
       
       def __init__(self):
           self.llm_endpoint = "http://localhost:11434/api/generate"  # Ollama
       
       async def select_venue(
           self,
           symbol: str,
           quantity: float,
           side: str,
           venues: List[Dict[str, Any]]
       ) -> str:
           """Use AI to select best venue"""
           
           # Build prompt
           prompt = f"""
           Select the best exchange for {side} order of {quantity} {symbol}.
           
           Venues:
           {self.format_venues(venues)}
           
           Consider:
           - Fill probability
           - Latency
           - Fees
           - Liquidity
           
           Return only the venue name.
           """
           
           # Query Ollama
           try:
               async with httpx.AsyncClient() as client:
                   response = await client.post(
                       self.llm_endpoint,
                       json={
                           "model": "llama3.2:3b",
                           "prompt": prompt,
                           "stream": False
                       },
                       timeout=1.0
                   )
                   response.raise_for_status()
                   result = response.json()
                   venue = result["response"].strip()
                   
                   logger.info(f"AI selected venue: {venue}")
                   return venue
           except Exception as e:
               logger.error(f"AI router error: {e}")
               # Fallback to rule-based selection
               return self.fallback_selection(venues)
       
       def format_venues(self, venues: List[Dict[str, Any]]) -> str:
           """Format venues for LLM"""
           return "\n".join([
               f"- {v['name']}: latency={v['latency_ms']}ms, fill_prob={v['fill_probability']}, fee={v['fee_pct']}%"
               for v in venues
           ])
       
       def fallback_selection(self, venues: List[Dict[str, Any]]) -> str:
           """Fallback to rule-based selection"""
           # Select venue with highest fill probability
           return max(venues, key=lambda v: v["fill_probability"])["name"]
   ```

**Deliverable**: Smart Order Router implemented with AI-driven selection

**Success Criteria**: SOR selects optimal venue, 30% better fill rates, latency considered

---

## ðŸ“‹ Phase 6: Hardware Timestamping and Monitoring - Week 8

### Task 6.1: Implement Hardware Timestamping

**Objective**: Add hardware timestamping for sub-microsecond latency measurement

**Actions for AI Agent**:

1. **Create timestamping service**:
   ```
   File: repo/monitor/src/timestamping/hardware_timestamp.rs
   Content:
   use std::time::{Duration, SystemTime, UNIX_EPOCH};
   
   pub struct HardwareTimestamp {
       pub nanoseconds: u64,
       pub source: TimestampSource,
   }
   
   pub enum TimestampSource {
       TSC,      // Time Stamp Counter (CPU)
       PTP,      // Precision Time Protocol (NIC)
       HPET,     // High Precision Event Timer
   }
   
   pub fn get_hardware_timestamp(source: TimestampSource) -> HardwareTimestamp {
       match source {
           TimestampSource::TSC => {
               // Read TSC register (x86_64)
               let tsc = unsafe { std::arch::x86_64::_rdtsc() };
               HardwareTimestamp {
                   nanoseconds: tsc,
                   source: TimestampSource::TSC,
               }
           }
           TimestampSource::PTP => {
               // Read PTP clock from NIC (requires driver support)
               // Placeholder: use system time
               let now = SystemTime::now()
                   .duration_since(UNIX_EPOCH)
                   .unwrap();
               HardwareTimestamp {
                   nanoseconds: now.as_nanos() as u64,
                   source: TimestampSource::PTP,
               }
           }
           TimestampSource::HPET => {
               // Read HPET (if available)
               let now = SystemTime::now()
                   .duration_since(UNIX_EPOCH)
                   .unwrap();
               HardwareTimestamp {
                   nanoseconds: now.as_nanos() as u64,
                   source: TimestampSource::HPET,
               }
           }
       }
   }
   
   pub fn measure_latency(start: HardwareTimestamp, end: HardwareTimestamp) -> Duration {
       Duration::from_nanos(end.nanoseconds.saturating_sub(start.nanoseconds))
   }
   ```

2. **Create latency heatmap**:
   ```
   File: repo/monitor/src/latency/heatmap.py
   Content:
   import numpy as np
   import matplotlib.pyplot as plt
   from typing import List, Tuple
   from collections import defaultdict
   
   class LatencyHeatmap:
       """Generate latency heatmaps for monitoring"""
       
       def __init__(self):
           self.latency_data = defaultdict(list)
       
       def record_latency(self, service: str, operation: str, latency_ns: int):
           """Record latency measurement"""
           key = f"{service}:{operation}"
           self.latency_data[key].append(latency_ns)
       
       def generate_heatmap(self, output_path: str):
           """Generate latency heatmap"""
           # Prepare data
           services = sorted(set(k.split(':')[0] for k in self.latency_data.keys()))
           operations = sorted(set(k.split(':')[1] for k in self.latency_data.keys()))
           
           # Create matrix
           matrix = np.zeros((len(services), len(operations)))
           
           for i, service in enumerate(services):
               for j, operation in enumerate(operations):
                   key = f"{service}:{operation}"
                   if key in self.latency_data:
                       # Use p95 latency
                       latencies = self.latency_data[key]
                       p95_index = int(len(latencies) * 0.95)
                       matrix[i, j] = sorted(latencies)[p95_index] / 1e6  # Convert to ms
           
           # Plot heatmap
           fig, ax = plt.subplots(figsize=(12, 8))
           im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto')
           
           ax.set_xticks(np.arange(len(operations)))
           ax.set_yticks(np.arange(len(services)))
           ax.set_xticklabels(operations)
           ax.set_yticklabels(services)
           
           plt.colorbar(im, label='Latency (ms)')
           plt.title('FKS Service Latency Heatmap')
           plt.tight_layout()
           plt.savefig(output_path)
           plt.close()
   ```

**Deliverable**: Hardware timestamping and latency monitoring implemented

**Success Criteria**: Timestamps accurate to <100ns, heatmaps generated, latency tracked

---

## ðŸ“‹ Phase 7: Event-Driven Pipelines (LMAX Disruptor) - Week 8

### Task 7.1: Implement Lock-Free Queue

**Objective**: Replace standard queues with LMAX Disruptor-style ring buffer

**Actions for AI Agent**:

1. **Create ring buffer in Rust**:
   ```
   File: repo/execution/src/disruptor/ring_buffer.rs
   Content:
   use std::sync::atomic::{AtomicUsize, Ordering};
   use std::marker::PhantomData;
   
   pub struct RingBuffer<T> {
       buffer: Vec<T>,
       size: usize,
       mask: usize,
       sequence: AtomicUsize,
       cursor: AtomicUsize,
   }
   
   impl<T: Clone + Default> RingBuffer<T> {
       pub fn new(size: usize) -> Self {
           // Size must be power of 2
           assert!(size > 0 && (size & (size - 1)) == 0);
           
           RingBuffer {
               buffer: vec![T::default(); size],
               size,
               mask: size - 1,
               sequence: AtomicUsize::new(0),
               cursor: AtomicUsize::new(0),
           }
       }
       
       pub fn publish(&self, item: T) -> usize {
           let sequence = self.sequence.fetch_add(1, Ordering::Acquire);
           let index = sequence & self.mask;
           
           // Wait for slot to be available (simple spin lock)
           while self.cursor.load(Ordering::Acquire) < sequence - self.size + 1 {
               std::hint::spin_loop();
           }
           
           self.buffer[index] = item;
           
           sequence
       }
       
       pub fn consume(&self, consumer_id: usize) -> Option<T> {
           let current = self.cursor.load(Ordering::Acquire);
           let next = current + 1;
           
           if next > self.sequence.load(Ordering::Acquire) {
               return None;  // No items available
           }
           
           let index = next & self.mask;
           let item = self.buffer[index].clone();
           
           self.cursor.store(next, Ordering::Release);
           
           Some(item)
       }
   }
   ```

2. **Integrate with fks_ai for signal processing**:
   ```
   File: repo/ai/src/disruptor/signal_processor.py
   Content:
   from typing import Optional
   from fks_execution.disruptor import RingBuffer
   
   class SignalProcessor:
       """Process signals using ring buffer"""
       
       def __init__(self, buffer_size: int = 1024):
           self.ring_buffer = RingBuffer(buffer_size)
           self.running = False
       
       def publish_signal(self, signal: dict):
           """Publish signal to ring buffer"""
           self.ring_buffer.publish(signal)
       
       def consume_signal(self) -> Optional[dict]:
           """Consume signal from ring buffer"""
           return self.ring_buffer.consume(0)
   ```

**Deliverable**: Lock-free ring buffer implemented

**Success Criteria**: No locks, <100ns publish/consume latency, jitter eliminated

---

## ðŸ“Š Success Metrics

### Performance Targets
- Data ingestion latency: <100ns (from 2-5ms) âœ… 50x improvement
- Signal generation: <1Î¼s with FPGA (from 10-50ms) âœ… 10,000x improvement
- Order execution: <500ns (from 5-10ms) âœ… 10,000x improvement
- End-to-end latency: <10Î¼s (from 20-70ms) âœ… 2,000x improvement
- System throughput: 1M+ trades/second âœ…

### Business Metrics
- Arbitrage detection: <500ns âœ…
- Fill rate improvement: 30% âœ…
- Latency reduction: 95% âœ…
- Power efficiency: 50% reduction vs GPU âœ…

---

## ðŸŽ¯ Implementation Checklist

### Phase 1: Preparation âœ…
- [ ] Latency profiling complete
- [ ] Bottlenecks identified
- [ ] Hardware requirements defined
- [ ] FPGA feasibility study complete

### Phase 2: Kernel-Bypass (DPDK) âœ…
- [ ] DPDK installed and configured
- [ ] fks_execution uses DPDK
- [ ] fks_data transmits via DPDK
- [ ] 40-50% latency reduction achieved

### Phase 3: FPGA Acceleration âœ…
- [ ] FPGA development environment set up
- [ ] Arbitrage detector implemented
- [ ] FPGA integrated with fks_ai
- [ ] <500ns latency verified

### Phase 4: In-Memory Order Books âœ…
- [ ] Lock-free order book implemented
- [ ] Integrated with fks_app
- [ ] 25% faster backtesting
- [ ] <100ns update latency

### Phase 5: Smart Order Router âœ…
- [ ] SOR implemented
- [ ] AI-driven venue selection
- [ ] 30% better fill rates
- [ ] Latency-aware routing

### Phase 6: Hardware Timestamping âœ…
- [ ] Hardware timestamping implemented
- [ ] Latency heatmaps generated
- [ ] <100ns accuracy
- [ ] Monitoring integrated

### Phase 7: Event-Driven Pipelines âœ…
- [ ] Ring buffer implemented
- [ ] Lock-free queues
- [ ] Jitter eliminated
- [ ] <100ns publish/consume

---

## ðŸ”§ Integration with Multi-Agent Bots

### HFT + Multi-Agent Integration Points

1. **FPGA + Bots**: FPGA detects arbitrage, bots validate with AI
2. **DPDK + Bots**: Ultra-low-latency data feeds to bots
3. **Order Book + Bots**: Real-time order book for bot signals
4. **SOR + Bots**: Bots use SOR for optimal execution
5. **Timestamping + Bots**: Measure bot decision latency

### Combined Workflow

```
Market Data (DPDK) â†’ FPGA (Arbitrage) â†’ Bots (Validation) â†’ Order Book â†’ SOR â†’ Execution
     <100ns            <500ns            <1Î¼s                <100ns      <200ns  <500ns
```

**Total Latency: <2.5Î¼s** (from 20-70ms) âœ… **28,000x improvement**

---

**This document provides complete, step-by-step instructions for AI agents to implement HFT optimizations. Follow tasks sequentially, ensuring all deliverables are created and success criteria are met.**

