# FKS AI - GPU ML/RAG Service Dockerfile
# Uses ML base image with LangChain, ChromaDB, and sentence-transformers pre-installed
# For GPU support, we extend the ML base with CUDA-enabled PyTorch
FROM nuniesmith/fks:docker-ml AS builder

WORKDIR /app

# ML packages (langchain, chromadb, sentence-transformers, ollama, TA-Lib) are already installed in base
# Install PyTorch with CUDA support (for GPU acceleration)
# Note: For CPU-only, we can skip this and use the ML base as-is
RUN --mount=type=cache,target=/root/.cache/pip \
    python -m pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cu121 || \
    python -m pip install --no-cache-dir torch || echo "PyTorch installation optional"

# Copy requirements (service-specific packages)
COPY requirements.txt requirements-langgraph.txt* ./

# Install service-specific Python dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    python -m pip install --user --no-warn-script-location --no-cache-dir -r requirements.txt && \
    if [ -f requirements-langgraph.txt ]; then \
        python -m pip install --user --no-warn-script-location --no-cache-dir -r requirements-langgraph.txt; \
    fi

# Runtime stage
FROM python:3.12-slim

# Environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    SERVICE_NAME=fks_ai \
    SERVICE_PORT=8007 \
    PYTHONPATH=/app/src:/app \
    PATH=/home/appuser/.local/bin:$PATH \
    TZ=America/Toronto

WORKDIR /app

# Install runtime dependencies only
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -u 1000 -m -s /bin/bash appuser

# Copy TA-Lib libraries from builder (needed at runtime)
COPY --from=builder /usr/lib/libta_lib.so* /usr/lib/ || true

# Copy Python packages from builder
COPY --from=builder --chown=appuser:appuser /root/.local /home/appuser/.local

# Copy application code
COPY --chown=appuser:appuser src/ ./src/
COPY --chown=appuser:appuser entrypoint.sh* ./

# Create directories for models and cache
RUN mkdir -p /home/appuser/.cache/huggingface /app/models && \
    chown -R appuser:appuser /home/appuser/.cache /app/models

# Make entrypoint executable if it exists
RUN if [ -f entrypoint.sh ]; then chmod +x entrypoint.sh; fi

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 --start-period=60s \
  CMD python -c "import os,urllib.request,sys;port=os.getenv('SERVICE_PORT','8007');u=f'http://localhost:{port}/health';\
import urllib.error;\
try: urllib.request.urlopen(u,timeout=3);\
except Exception: sys.exit(1)" || exit 1

# Expose port
EXPOSE 8007

# Use entrypoint script if available, otherwise run directly
ENTRYPOINT ["./entrypoint.sh"]
