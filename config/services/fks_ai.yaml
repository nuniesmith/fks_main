# FKS AI Service Configuration
# Central config location: repo/main/config/services/fks_ai.yaml

service:
  name: fks_ai
  port: 8007
  host: 0.0.0.0
  environment: ${ENVIRONMENT:-development}
  log_level: ${LOG_LEVEL:-INFO}

# Dependencies from service_registry.json
dependencies:
  fks_data:
    base_url: http://fks_data:8003
    health_url: http://fks_data:8003/health

database:
  host: ${POSTGRES_HOST:-postgres}
  port: ${POSTGRES_PORT:-5432}
  name: ${POSTGRES_DB:-trading_db}
  user: ${POSTGRES_USER:-postgres}
  password: ""  # Use env var: POSTGRES_PASSWORD

redis:
  host: ${REDIS_HOST:-redis}
  port: ${REDIS_PORT:-6379}
  db: 3
  password: ""  # Use env var: REDIS_PASSWORD

api:
  timeout: 120  # Longer timeout for AI operations
  retries: 2
  rate_limit: 50

monitoring:
  enabled: true
  prometheus_port: 9090
  health_check_interval: 30

paths:
  logs: /app/logs  # Inside container
  models: /app/models
  cache: /app/cache

features:
  local_llm: ${USE_LOCAL_LLM:-false}
  rag: true
  multi_agent: true
  signal_refinement: true

service_specific:
  llm:
    provider: ${LLM_PROVIDER:-ollama}  # ollama, openai, google
    model: ${LLM_MODEL:-llama3}
    temperature: 0.7
    max_tokens: 2000
  ollama:
    host: ${OLLAMA_HOST:-http://ollama:11434}
    timeout: 60
  openai:
    api_key: ""  # Use env var: OPENAI_API_KEY
    model: gpt-4
  google:
    api_key: ""  # Use env var: GOOGLE_AI_API_KEY
    model: gemini-pro
  agents:
    analyzer:
      enabled: true
      model: ${ANALYZER_MODEL:-llama3}
    debater:
      enabled: true
      model: ${DEBATER_MODEL:-llama3}
    signal_processor:
      enabled: true
      model: ${SIGNAL_MODEL:-llama3}

